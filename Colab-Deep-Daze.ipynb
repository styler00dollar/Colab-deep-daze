{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-deep-daze",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMOyHqy8wt1H"
      },
      "source": [
        "# Colab-deep-daze\r\n",
        "Original repo: [lucidrains/deep-daze](https://github.com/lucidrains/deep-daze)\r\n",
        "\r\n",
        "My fork: [styler00dollar/Colab-deep-daze](https://github.com/styler00dollar/Colab-deep-daze)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3u8c3IpMld"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55YJsbFV10i8",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "! pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "!pip install deep-daze"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cskITQNpwEW6"
      },
      "source": [
        "# Using Google Drive and disabling heavy printing to avoid output size limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCBTrL3XqXf-",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "print('Google Drive connected.')\r\n",
        "!mkdir '/content/drive/MyDrive/DeepDaze/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "CKQpTiTFx9RU"
      },
      "source": [
        "#@title removing printing\r\n",
        "%%writefile /usr/local/lib/python3.6/dist-packages/deep_daze/deep_daze.py\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from random import sample\r\n",
        "from torch import nn\r\n",
        "from torch.optim import Adam\r\n",
        "from torch.cuda.amp import GradScaler, autocast\r\n",
        "\r\n",
        "from pathlib import Path\r\n",
        "from tqdm import trange\r\n",
        "import torchvision\r\n",
        "\r\n",
        "from deep_daze.clip import load, tokenize, normalize_image\r\n",
        "from siren_pytorch import SirenNet, SirenWrapper\r\n",
        "\r\n",
        "from collections import namedtuple\r\n",
        "from einops import rearrange\r\n",
        "\r\n",
        "assert torch.cuda.is_available(), 'CUDA must be available in order to use Deep Daze'\r\n",
        "\r\n",
        "# helpers\r\n",
        "\r\n",
        "def exists(val):\r\n",
        "    return val is not None\r\n",
        "\r\n",
        "def interpolate(image, size):\r\n",
        "    return F.interpolate(image, (size, size), mode = 'bilinear', align_corners = False)\r\n",
        "\r\n",
        "def rand_cutout(image, size):\r\n",
        "    width = image.shape[-1]\r\n",
        "    offsetx = torch.randint(0, width - size, ())\r\n",
        "    offsety = torch.randint(0, width - size, ())\r\n",
        "    cutout = image[:, :, offsetx:offsetx + size, offsety:offsety + size]\r\n",
        "    return cutout\r\n",
        "\r\n",
        "# load clip\r\n",
        "\r\n",
        "perceptor, preprocess = load()\r\n",
        "\r\n",
        "# load siren\r\n",
        "\r\n",
        "def norm_siren_output(img):\r\n",
        "    return (img.tanh() + 1) * 0.5\r\n",
        "\r\n",
        "class DeepDaze(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        total_batches,\r\n",
        "        batch_size,\r\n",
        "        num_layers = 8,\r\n",
        "        image_width = 512,\r\n",
        "        loss_coef = 100,\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.loss_coef = loss_coef\r\n",
        "        self.image_width = image_width\r\n",
        "\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.total_batches = total_batches\r\n",
        "        self.num_batches_processed = 0\r\n",
        "\r\n",
        "        siren = SirenNet(\r\n",
        "            dim_in = 2,\r\n",
        "            dim_hidden = 256,\r\n",
        "            num_layers = num_layers,\r\n",
        "            dim_out = 3,\r\n",
        "            use_bias = True\r\n",
        "        )\r\n",
        "\r\n",
        "        self.model = SirenWrapper(\r\n",
        "            siren,\r\n",
        "            image_width = image_width,\r\n",
        "            image_height = image_width\r\n",
        "        )\r\n",
        "\r\n",
        "        self.generate_size_schedule()\r\n",
        "\r\n",
        "    def forward(self, text, return_loss = True):\r\n",
        "        out = self.model()\r\n",
        "        out = norm_siren_output(out)\r\n",
        "\r\n",
        "        if not return_loss:\r\n",
        "            return out\r\n",
        "\r\n",
        "        pieces = []\r\n",
        "        width = out.shape[-1]\r\n",
        "        size_slice = slice(self.num_batches_processed, self.num_batches_processed + self.batch_size)\r\n",
        "\r\n",
        "        for size in self.scheduled_sizes[size_slice]:\r\n",
        "            apper = rand_cutout(out, size)\r\n",
        "            apper = interpolate(apper, 224)\r\n",
        "            pieces.append(normalize_image(apper))\r\n",
        "\r\n",
        "        image = torch.cat(pieces)\r\n",
        "\r\n",
        "        with autocast(enabled = False):\r\n",
        "            image_embed = perceptor.encode_image(image)\r\n",
        "            text_embed = perceptor.encode_text(text)\r\n",
        "\r\n",
        "        self.num_batches_processed += self.batch_size\r\n",
        "\r\n",
        "        loss = -self.loss_coef * torch.cosine_similarity(text_embed, image_embed, dim = -1).mean()\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def generate_size_schedule(self):\r\n",
        "        batches = 0\r\n",
        "        counter = 0\r\n",
        "        self.scheduled_sizes = []\r\n",
        "\r\n",
        "        while batches < self.total_batches:\r\n",
        "            counter += 1\r\n",
        "            sizes = self.sample_sizes(counter)\r\n",
        "            batches += len(sizes)\r\n",
        "            self.scheduled_sizes.extend(sizes)\r\n",
        "\r\n",
        "    def sample_sizes(self, counter):\r\n",
        "        pieces_per_group = 4\r\n",
        "\r\n",
        "        # 6 piece schedule increasing in context as model saturates\r\n",
        "        if counter < 500:\r\n",
        "            partition = [4,5,3,2,1,1]\r\n",
        "        elif counter < 1000:\r\n",
        "            partition = [2,5,4,2,2,1]\r\n",
        "        elif counter < 1500:\r\n",
        "            partition = [1,4,5,3,2,1]\r\n",
        "        elif counter < 2000:\r\n",
        "            partition = [1,3,4,4,2,2]\r\n",
        "        elif counter < 2500:\r\n",
        "            partition = [1,2,2,4,4,3]\r\n",
        "        elif counter < 3000:\r\n",
        "            partition = [1,1,2,3,4,5]\r\n",
        "        else:\r\n",
        "            partition = [1,1,1,2,4,7]\r\n",
        "\r\n",
        "        dbase = .38\r\n",
        "        step = .1\r\n",
        "        width = self.image_width\r\n",
        "\r\n",
        "        sizes = []\r\n",
        "        for part_index in range(len(partition)):\r\n",
        "            groups = partition[part_index]\r\n",
        "            for _ in range(groups * pieces_per_group):\r\n",
        "                sizes.append(torch.randint(\r\n",
        "                    int((dbase + step * part_index + .01) * width),\r\n",
        "                    int((dbase + step * (1 + part_index)) * width), ()))\r\n",
        "\r\n",
        "        sizes.sort()\r\n",
        "        return sizes\r\n",
        "\r\n",
        "\r\n",
        "class Imagine(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        text,\r\n",
        "        *,\r\n",
        "        lr = 1e-5,\r\n",
        "        batch_size = 4,\r\n",
        "        gradient_accumulate_every = 4,\r\n",
        "        save_every = 100,\r\n",
        "        image_width = 512,\r\n",
        "        num_layers = 16,\r\n",
        "        epochs = 20,\r\n",
        "        iterations = 1050\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.epochs = epochs\r\n",
        "        self.iterations = iterations\r\n",
        "        total_batches = epochs * iterations * batch_size * gradient_accumulate_every\r\n",
        "\r\n",
        "        model = DeepDaze(\r\n",
        "            total_batches = total_batches,\r\n",
        "            batch_size = batch_size,\r\n",
        "            image_width = image_width,\r\n",
        "            num_layers = num_layers\r\n",
        "        ).cuda()\r\n",
        "\r\n",
        "        self.model = model\r\n",
        "\r\n",
        "        self.scaler = GradScaler()\r\n",
        "        self.optimizer = Adam(model.parameters(), lr)\r\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\r\n",
        "        self.save_every = save_every\r\n",
        "\r\n",
        "        self.text = text\r\n",
        "        textpath = self.text.replace(' ','_')\r\n",
        "        self.filename = Path(f'./{textpath}.png')\r\n",
        "\r\n",
        "        self.encoded_text = tokenize(text).cuda()\r\n",
        "\r\n",
        "    def train_step(self, epoch, i):\r\n",
        "\r\n",
        "        for _ in range(self.gradient_accumulate_every):\r\n",
        "            with autocast():\r\n",
        "                loss = self.model(self.encoded_text)\r\n",
        "            self.scaler.scale(loss / self.gradient_accumulate_every).backward()\r\n",
        "\r\n",
        "        self.scaler.step(self.optimizer)\r\n",
        "        self.scaler.update()\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "\r\n",
        "        if i % self.save_every == 0:\r\n",
        "            with torch.no_grad():\r\n",
        "                al = normalize_image(self.model(self.encoded_text, return_loss = False).cpu())\r\n",
        "                torchvision.utils.save_image(al, str(self.filename))\r\n",
        "                #print(f'image updated at \"./{str(self.filename)}\"')\r\n",
        "\r\n",
        "    def forward(self):\r\n",
        "        print(f'Imagining \"{self.text}\" from the depths of my weights...')\r\n",
        "\r\n",
        "        for epoch in trange(self.epochs, desc = 'epochs'):\r\n",
        "            for i in trange(self.iterations, desc='iteration'):\r\n",
        "                self.train_step(epoch, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy2SvlEMxHVH"
      },
      "source": [
        "Current default is ```!mkdir '/content/drive/MyDrive/DeepDaze/'```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IPQ_rdA2Sa7",
        "cellView": "form"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from IPython.display import Image, display\n",
        "import shutil\n",
        "\n",
        "from deep_daze import Imagine\n",
        "\n",
        "save_path = '/content/drive/MyDrive/DeepDaze/' #@param {type:\"string\"}\n",
        "TEXT = 'happy anime character' #@param {type:\"string\"}\n",
        "TEXT_PATH = TEXT.replace(\" \", \"_\")\n",
        "NUM_LAYERS = 32 #@param {type:\"number\"}\n",
        "\n",
        "model = Imagine(\n",
        "    text = TEXT,\n",
        "    num_layers = NUM_LAYERS,\n",
        "    save_every = 50 #@param {type:\"number\"}\n",
        ")\n",
        "\n",
        "# without heavy printing and saving images to drive\n",
        "for epoch in range(20):\n",
        "    for i in range(1000):\n",
        "        model.train_step(epoch, i)\n",
        "\n",
        "        if i % model.save_every != 0:\n",
        "            continue\n",
        "\n",
        "        file_end = str(\"_\"+str({epoch})+\"_\"+str({i})+\".png\")\n",
        "        source = f'{TEXT_PATH}.png'\n",
        "        destination = f'{save_path}{TEXT_PATH}{file_end}'\n",
        "        print(source, destination)\n",
        "        shutil.move(source, destination)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96Ci-EZwQ-_"
      },
      "source": [
        "# Original with trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU7XY-2PwTAD"
      },
      "source": [
        "Warning: In this current state you will get ```Buffered data was truncated after reaching the output size limit.``` after a while and thus dont see further progress. Combined with Colabs terminating behaviour and risking to loose the output with missing printing, this is not really a good idea to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFVG3rGVp-_S",
        "cellView": "form"
      },
      "source": [
        "from tqdm import trange\r\n",
        "from IPython.display import Image, display\r\n",
        "\r\n",
        "from deep_daze import Imagine\r\n",
        "\r\n",
        "TEXT = 'an apple next to a fireplace' #@param {type:\"string\"}\r\n",
        "NUM_LAYERS = 32 #@param {type:\"number\"}\r\n",
        "\r\n",
        "model = Imagine(\r\n",
        "    text = TEXT,\r\n",
        "    num_layers = NUM_LAYERS,\r\n",
        "    save_every = 50\r\n",
        ")\r\n",
        "\r\n",
        "for epoch in trange(20, desc = 'epochs'):\r\n",
        "    for i in trange(1000, desc = 'iteration'):\r\n",
        "        model.train_step(epoch, i)\r\n",
        "\r\n",
        "        if i % model.save_every != 0:\r\n",
        "            continue\r\n",
        "\r\n",
        "        image = Image(f'./{TEXT}.png')\r\n",
        "        display(image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
